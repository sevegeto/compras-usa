# Compras USA - Web Scraper

Script avanzado para scrapear m√∫ltiples sitios web de compras (GunMagWarehouse, eBay, Amazon, Academy) y guardar autom√°ticamente los datos en Google Sheets.

## üöÄ Caracter√≠sticas

- ‚úÖ **Scraping de m√∫ltiples sitios**: Soporta GunMagWarehouse, eBay, Amazon y Academy
- ‚úÖ **Integraci√≥n con Google Sheets**: Guarda autom√°ticamente los productos extra√≠dos
- ‚úÖ **Configuraci√≥n mediante variables de entorno**: Sin rutas hardcodeadas
- ‚úÖ **Sistema de logging robusto**: Registro detallado en archivo y consola
- ‚úÖ **Arquitectura modular**: F√°cil de extender con nuevos scrapers
- ‚úÖ **Manejo de errores**: Reintentos autom√°ticos y manejo graceful de fallos
- ‚úÖ **Modo headless**: Ejecuta sin interfaz gr√°fica para mayor eficiencia
- ‚úÖ **Type hints**: C√≥digo documentado con tipos para mejor mantenibilidad

## üìã Requisitos

### Software Necesario

- **Python 3.8+**
- **Google Chrome** (instalado en el sistema)
- **ChromeDriver** compatible con tu versi√≥n de Chrome
- **Cuenta de servicio de Google** con acceso a Google Sheets API

### Dependencias de Python

Las dependencias se instalan autom√°ticamente desde `requirements.txt`:

```
selenium>=4.0.0
beautifulsoup4>=4.11.0
gspread>=5.7.0
oauth2client>=4.1.3
python-dotenv>=1.0.0
```

## üîß Instalaci√≥n

### 1. Clonar el Repositorio

```bash
git clone https://github.com/sevegeto/compras-usa.git
cd compras-usa
```

### 2. Crear Entorno Virtual (Recomendado)

```bash
python -m venv venv

# Windows
venv\Scripts\activate

# Linux/Mac
source venv/bin/activate
```

### 3. Instalar Dependencias

```bash
pip install -r requirements.txt
```

### 4. Descargar ChromeDriver

1. Verifica tu versi√≥n de Chrome: `chrome://version/`
2. Descarga ChromeDriver compatible desde: https://chromedriver.chromium.org/downloads
3. Coloca el ejecutable en una ubicaci√≥n accesible

### 5. Configurar Google Sheets API

1. Ve a [Google Cloud Console](https://console.cloud.google.com/)
2. Crea un nuevo proyecto o selecciona uno existente
3. Habilita **Google Sheets API** y **Google Drive API**
4. Crea una cuenta de servicio:
   - Ve a "IAM y administraci√≥n" > "Cuentas de servicio"
   - Crea una nueva cuenta de servicio
   - Descarga el archivo JSON de credenciales
5. Comparte tu Google Sheet con el email de la cuenta de servicio

### 6. Configurar Variables de Entorno

Copia el archivo de ejemplo y config√∫ralo:

```bash
cp .env.example .env
```

Edita `.env` con tus valores:

```env
# ChromeDriver Configuration
CHROMEDRIVER_PATH=/ruta/a/chromedriver

# Google Sheets Configuration
GOOGLE_SHEET_ID=tu_id_de_google_sheet
SHEET_NAME=Compras
CREDENTIALS_PATH=/ruta/a/credenciales.json

# Scraping Configuration
HEADLESS_MODE=True
PAGE_LOAD_TIMEOUT=30
IMPLICIT_WAIT=10

# Logging
LOG_LEVEL=INFO
LOG_FILE=scraper.log
```

**Obtener el GOOGLE_SHEET_ID:**
De la URL de tu Google Sheet: `https://docs.google.com/spreadsheets/d/[ESTE_ES_EL_ID]/edit`

## üìñ Uso

### Uso B√°sico

```python
from scraper import WebScraperOrchestrator

# Definir URLs a scrapear
urls = {
    'ebay': 'https://www.ebay.com/sch/i.html?_nkw=electronics',
    'amazon': 'https://www.amazon.com/s?k=electronics',
}

# Ejecutar scraper
orquestador = WebScraperOrchestrator()
productos = orquestador.ejecutar(urls, guardar=True)
```

### Ejecutar el Script Principal

```bash
python scraper.py
```

### Personalizar URLs

Edita la funci√≥n `main()` en `scraper.py`:

```python
urls_ejemplo = {
    'ebay': 'https://www.ebay.com/sch/i.html?_nkw=tu_busqueda',
    'amazon': 'https://www.amazon.com/s?k=tu_busqueda',
    'gunmagwarehouse': 'https://gunmagwarehouse.com/categoria',
    'academy': 'https://www.academy.com/shop/categoria'
}
```

## üèóÔ∏è Arquitectura

### Estructura del Proyecto

```
compras-usa/
‚îú‚îÄ‚îÄ scraper.py              # Script principal
‚îú‚îÄ‚îÄ requirements.txt        # Dependencias
‚îú‚îÄ‚îÄ .env.example           # Plantilla de configuraci√≥n
‚îú‚îÄ‚îÄ .gitignore             # Archivos ignorados por Git
‚îú‚îÄ‚îÄ README.md              # Esta documentaci√≥n
‚îî‚îÄ‚îÄ scraper.log            # Logs generados (se crea autom√°ticamente)
```

### Componentes Principales

- **GoogleSheetsManager**: Gestiona la conexi√≥n y escritura en Google Sheets
- **SeleniumDriver**: Configura y gestiona el driver de Selenium
- **BaseScraper**: Clase base para todos los scrapers
- **[Sitio]Scraper**: Scrapers espec√≠ficos para cada sitio web
- **WebScraperOrchestrator**: Coordina todo el proceso de scraping

### Flujo de Ejecuci√≥n

1. **Validaci√≥n de Configuraci√≥n**: Verifica que todas las variables est√©n configuradas
2. **Inicializaci√≥n**: Crea instancias de driver, scrapers y conexi√≥n a Sheets
3. **Scraping**: Extrae datos de cada sitio especificado
4. **Almacenamiento**: Guarda los productos en Google Sheets
5. **Limpieza**: Cierra conexiones y libera recursos

## üîç Logs y Debugging

Los logs se guardan en:
- **Archivo**: `scraper.log` (nivel DEBUG)
- **Consola**: stdout (nivel INFO)

Formato de log:
```
2026-01-10 12:00:00 - web_scraper - INFO - Mensaje del log
```

Cambiar nivel de logging en `.env`:
```env
LOG_LEVEL=DEBUG  # DEBUG, INFO, WARNING, ERROR, CRITICAL
```

## üõ†Ô∏è Extender con Nuevos Scrapers

Para agregar un nuevo sitio web:

1. Crea una nueva clase que herede de `BaseScraper`:

```python
class NuevoSitioScraper(BaseScraper):
    def __init__(self, driver: webdriver.Chrome):
        super().__init__(driver)
        self.nombre_sitio = "NuevoSitio"
    
    def extraer_datos(self, url: str) -> List[Dict[str, str]]:
        productos = []
        # Implementar l√≥gica de scraping
        return productos
```

2. Registra el scraper en `WebScraperOrchestrator.inicializar()`:

```python
self.scrapers = {
    # ... scrapers existentes ...
    'nuevositio': NuevoSitioScraper(driver)
}
```

## ‚ö†Ô∏è Consideraciones Importantes

### Legales y √âticas

- ‚úÖ Respeta los t√©rminos de servicio de cada sitio web
- ‚úÖ No realices scraping excesivo que pueda afectar al sitio
- ‚úÖ Considera agregar delays entre requests
- ‚úÖ Verifica si los sitios ofrecen APIs oficiales

### Limitaciones

- Los selectores CSS/HTML pueden cambiar cuando los sitios actualizan su dise√±o
- Algunos sitios pueden bloquear bots (usar headers y delays apropiados)
- El modo headless puede ser detectado por algunos sitios

### Mantenimiento

- Actualiza ChromeDriver cuando actualices Chrome
- Revisa los logs regularmente para detectar fallos
- Actualiza los selectores si los sitios cambian su estructura

## üîí Seguridad

- ‚ùå **NUNCA** commits archivos de credenciales (`credenciales.json`)
- ‚ùå **NUNCA** commits el archivo `.env` con valores reales
- ‚úÖ Usa `.gitignore` para excluir archivos sensibles
- ‚úÖ Usa variables de entorno para configuraci√≥n
- ‚úÖ Rota las credenciales peri√≥dicamente

## üêõ Troubleshooting

### Error: ChromeDriver no encontrado

```bash
# Verifica que la ruta en .env sea correcta
# Windows: CHROMEDRIVER_PATH=C:\\ruta\\a\\chromedriver.exe
# Linux/Mac: CHROMEDRIVER_PATH=/ruta/a/chromedriver
```

### Error: No se puede conectar a Google Sheets

1. Verifica que el archivo de credenciales existe
2. Confirma que las APIs est√°n habilitadas
3. Verifica que compartiste la hoja con la cuenta de servicio

### Error: Selectores no encuentran elementos

Los sitios web cambian frecuentemente. Inspecciona la p√°gina y actualiza los selectores en el scraper correspondiente.

## üìù Licencia

Este proyecto es de c√≥digo abierto. √ösalo responsablemente.

## ü§ù Contribuciones

Las contribuciones son bienvenidas:

1. Fork el proyecto
2. Crea una rama para tu feature (`git checkout -b feature/nueva-funcionalidad`)
3. Commit tus cambios (`git commit -m 'Agrega nueva funcionalidad'`)
4. Push a la rama (`git push origin feature/nueva-funcionalidad`)
5. Abre un Pull Request

## üìß Soporte

Para problemas o preguntas, abre un issue en el repositorio.

---

**Nota**: Este scraper est√° dise√±ado para prop√≥sitos educativos y de automatizaci√≥n personal. √ösalo de manera responsable y √©tica.
