# Compras USA - Web Scraper

Sistema automatizado de web scraping que lee URLs desde Google Sheets y extrae informaciÃ³n de productos de mÃºltiples sitios web.

## ğŸš€ CaracterÃ­sticas

âœ… **Lee URLs desde Google Sheets** - Columna H (a partir de H3) en la hoja "Compras"  
âœ… **DetecciÃ³n automÃ¡tica de dominio** - Soporta GunMagWarehouse, eBay, Amazon, Academy  
âœ… **Selenium para contenido dinÃ¡mico** - Carga JavaScript y contenido asÃ­ncrono  
âœ… **ExtracciÃ³n completa de datos** - Nombre, Precio, DescripciÃ³n, Imagen, Link  
âœ… **Escritura automÃ¡tica a Sheets** - Inserta los datos extraÃ­dos en Google Sheets  
âœ… **FÃ¡cilmente extensible** - Estructura modular para aÃ±adir nuevos sitios

## ğŸ“‹ Requisitos

- Python 3.8 o superior
- Google Chrome/Chromium instalado
- Cuenta de Google Cloud con Google Sheets API habilitada

## ğŸ”§ InstalaciÃ³n

1. **Clonar el repositorio**
```bash
git clone https://github.com/sevegeto/compras-usa.git
cd compras-usa
```

2. **Instalar dependencias**
```bash
pip install -r requirements.txt
```

3. **Configurar Google Sheets API**

   a. Ve a [Google Cloud Console](https://console.cloud.google.com/)
   
   b. Crea un nuevo proyecto o selecciona uno existente
   
   c. Habilita la API de Google Sheets:
      - Ve a "APIs & Services" > "Library"
      - Busca "Google Sheets API"
      - Haz clic en "Enable"
   
   d. Crea credenciales OAuth 2.0:
      - Ve a "APIs & Services" > "Credentials"
      - Clic en "Create Credentials" > "OAuth client ID"
      - Selecciona "Desktop app" como tipo de aplicaciÃ³n
      - Descarga el archivo JSON
   
   e. Renombra el archivo descargado a `credentials.json` y colÃ³calo en el directorio raÃ­z del proyecto

## ğŸ“– Uso

### Uso BÃ¡sico

```bash
python main.py YOUR_SPREADSHEET_ID
```

Donde `YOUR_SPREADSHEET_ID` es el ID de tu hoja de Google Sheets (se encuentra en la URL de la hoja).

### Opciones Avanzadas

```bash
python main.py YOUR_SPREADSHEET_ID \
  --sheet "Compras" \
  --column "H" \
  --start-row 3 \
  --headless
```

**ParÃ¡metros:**
- `spreadsheet_id` (requerido): ID de la hoja de Google Sheets
- `--sheet`: Nombre de la hoja (default: "Compras")
- `--column`: Columna que contiene las URLs (default: "H")
- `--start-row`: Fila inicial (default: 3)
- `--headless`: Ejecutar el navegador en modo headless (default: True)

### Primera EjecuciÃ³n

En la primera ejecuciÃ³n, se abrirÃ¡ una ventana del navegador para autenticar tu cuenta de Google. El token se guardarÃ¡ en `token.json` para ejecuciones futuras.

## ğŸ“Š Estructura de Google Sheets

El script espera la siguiente estructura:

**Entrada (Columna H):**
- URLs de productos (una por fila, empezando en H3)

**Salida (Columnas A-E):**
- **Columna A:** Nombre del producto
- **Columna B:** Precio
- **Columna C:** DescripciÃ³n
- **Columna D:** URL de la imagen
- **Columna E:** Link del producto

## ğŸŒ Sitios Soportados

- **GunMagWarehouse** (gunmagwarehouse.com)
- **eBay** (ebay.com)
- **Amazon** (amazon.com)
- **Academy Sports + Outdoors** (academy.com)

## ğŸ”Œ AÃ±adir Nuevos Sitios

Para aÃ±adir soporte para un nuevo sitio web:

1. **Crear un nuevo scraper en `scrapers.py`:**

```python
class NuevoSitioScraper(BaseScraper):
    """Scraper para NuevoSitio.com"""
    
    def _scrape_product(self, url):
        self.driver.get(url)
        time.sleep(2)
        
        soup = self._get_soup()
        
        # Extraer datos usando selectores CSS especÃ­ficos del sitio
        name = soup.select_one('.product-title').get_text()
        price = soup.select_one('.price').get_text()
        # ... mÃ¡s extracciones
        
        return {
            'name': self._clean_text(name),
            'price': self._clean_price(price),
            'description': description,
            'image': image,
            'link': url
        }
```

2. **AÃ±adir el dominio a `domain_detector.py`:**

```python
DOMAIN_MAPPING = {
    # ... dominios existentes
    'nuevositio.com': 'nuevositio',
}

# Y en get_scraper_class():
scraper_map = {
    # ... scrapers existentes
    'nuevositio': NuevoSitioScraper,
}
```

## ğŸ“ Estructura del Proyecto

```
compras-usa/
â”œâ”€â”€ main.py              # Script principal
â”œâ”€â”€ scrapers.py          # Scrapers especÃ­ficos por sitio
â”œâ”€â”€ domain_detector.py   # DetecciÃ³n automÃ¡tica de dominios
â”œâ”€â”€ sheets_client.py     # Cliente de Google Sheets API
â”œâ”€â”€ requirements.txt     # Dependencias de Python
â”œâ”€â”€ credentials.json     # Credenciales de Google (no incluido)
â”œâ”€â”€ token.json          # Token de autenticaciÃ³n (generado)
â””â”€â”€ README.md           # Este archivo
```

## ğŸ› ï¸ Desarrollo

### Ejecutar en modo no-headless (ver el navegador)

```bash
python main.py YOUR_SPREADSHEET_ID --no-headless
```

### DepuraciÃ³n

El script imprime informaciÃ³n detallada durante la ejecuciÃ³n:
- URLs encontradas
- Dominio detectado
- Datos extraÃ­dos
- Errores encontrados

## âš ï¸ Notas Importantes

1. **Rate Limiting:** Algunos sitios pueden bloquear requests frecuentes. Usa el script con moderaciÃ³n.

2. **Estructura de sitios web:** Los selectores CSS pueden cambiar cuando los sitios actualizan su diseÃ±o. Actualiza los scrapers segÃºn sea necesario.

3. **Credenciales:** Nunca compartas tus archivos `credentials.json` o `token.json`.

4. **TÃ©rminos de servicio:** AsegÃºrate de cumplir con los tÃ©rminos de servicio de los sitios web que estÃ¡s scrapeando.

## ğŸ¤ Contribuciones

Las contribuciones son bienvenidas. Para aÃ±adir soporte para nuevos sitios o mejorar los scrapers existentes, por favor:

1. Fork el repositorio
2. Crea una rama para tu feature (`git checkout -b feature/nuevo-sitio`)
3. Commit tus cambios (`git commit -am 'AÃ±adir soporte para NuevoSitio'`)
4. Push a la rama (`git push origin feature/nuevo-sitio`)
5. Crea un Pull Request

## ğŸ“ Licencia

Este proyecto es de cÃ³digo abierto y estÃ¡ disponible bajo la licencia MIT.

## ğŸ“§ Contacto

Para preguntas o soporte, por favor abre un issue en GitHub.
